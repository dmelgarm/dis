%Chapter 6: Conclusions

\chapter{Conclusions and Future Outlook}

The unifying theme throughout this dissertation has been that the optimal combination of geophysical data sets provides faster and more robust images of the seismic source and that this int urn can be used for hazards mitigation, especially for rapid assessment of the tsunami source.

In Chapter 2 with data from the $M_w$7.2 April 4, 2010 El Mayor-Cucapah earthquake in northern Baja California we demonstrated convincingly that regional high-rate real-time GPS networks can become an essential tool for early warning and rapid earthquake response. We took this one step further by applying a Kalman filter to obtain an optimal combination of data from collocated high-rate GPS and very-high-rate accelerometer stations, which takes advantage of the strengths and minimizes the weaknesses of both data types. The result is a new methodology for providing displacement time series with millimeter precision which can be used to better characterize the seismic source. It provides an improved broadband record of ground displacements and velocities over the full range of frequencies sampled by the accelerometer data, as well as the static deformation. This approach is capable of precisely detecting P-wave arrivals of medium to large near-source earthquakes with dense networks of collocated GPS and seismic instruments, making it particularly valuable for earthquake early warning systems and rapid earthquake response. It also provides a powerful new analysis tool to study the rupture characteristics of large earthquakes.

We also showed that automatic baseline correction schemes from accelerometer data alone can, for selected stations, provide useful displacement information. However, the static offsets determined by such automated schemes are often unreliable. Furthermore, adding a static field constraint to the baseline correction procedure does not guarantee convergence to the correct value and even if the correct static offset is obtained; other parts of the waveform might be in error by large amounts. The objective estimation of baseline corrected displacements, even when incorporating reliable and independent information such as the static field estimated from GPS remains elusive. We have demonstrated that such a scheme, which eliminates the need for subjective judgment, is still fraught with problems. We tested the assumption that baseline corrections introduce errors only at long periods outside what is of engineering interest and find it does not hold for the Tohoku-oki data set. Analysis of the power spectral densities and displacement response spectra demonstrate that the frequency domain error incurred by the baseline correction is well within the frequency range of interest to both seismology and engineering. The Kalman filter method, which combines high rate GPS and accelerometer data, is not affected by these problems. We demonstrated that even for the K-net accelerometer data set which displays behavior associated with large baseline offsets the Kalman filter solution routinely produces reliable broadband displacements.

In Chapter 3 we discussed the issue of rapid modeling of the earthquake source with static field data. We presented an approach for real-time computation of centroid moment tensors for large events from local and regional GPS displacement records. We used the coseismic offsets and Green’s functions for a layered Earth to relate the deformation at the surface with source parameters at depth. We demonstrated the algorithm with two test cases, the 2010 $M_w$ 7.2 El Mayor-Cucapah earthquake and the 2003 $M_w$ 8.3 Tokachi-oki earthquake. In both cases we have shown that provided with low latency access to displacement data it is feasible to obtain a robust centroid location and moment tensor solution within the first 2-3 minutes after rupture initiation. The algorithm is computationally efficient and thus amenable for rapid modeling and early detection.

Also in Chapter 3 we discussed the application of the CMT methodology and of a static slip inversion to the $M_w$ 9 Tohoku-oki event. Considering the review of the sequence of events at the USGS National Earthquake Information Center (NEIC) \citep{hayes2011}, there are several stages where the independent information provided by the seismogeodetic time series could have been helpful for a great event such as Tohoku-oki. Both the NEIC and the Pacific Tsunami Warning Center obtained preliminary teleseismic $P$ wave locations and mechanisms within 5 minutes of the origin time, but the first public release was delayed until 9.7 minutes. Independent confirmation of the source size would provide confidence for releasing information earlier. The first two versions of ShakeMap and PAGER alerts were released with the affected areas based on point sources. The ShakeMap and Pager alerts were updated to a finite fault source, significantly extending the length of affected coastline after 2 hours and 42 minutes. The initial result of the line source \textit{fast}CMT and slip inversion solutions reproduced the main features of the rupture, approximately 340 km long with average slip of 15 m over the main source region. Early access to this information, which in addition had the correct fault orientation of 204$^\circ$ as opposed to 230$^\circ$ for the rapid finite fault inversion based on the W phase fault orientation, could provide confidence for earlier release of the finite fault Global ShakeMap. Full integration and combination of GPS into seismic monitoring is essential for a hazard system that is more robust than either traditional seismic or geodetic monitoring alone. Because the \textit{fast}CMT algorithm does not require assumptions about the fault geometry, it is also applicable to source regions outside subduction zones. The ability to produce seismogeodetic displacement and velocity time series in real time implies that the delay for calculating near-real-time kinematic rupture models can also be significantly reduced.

In Chapter 4 we further elaborate on the kinematic problem. We show that the seismogeodetic data can be easily employed to obtain a time dependent rupture model of the Tohoku-oki source. Inversion of 3 component seismogeodetic data fro 20 stations revealed features of the source which largely agree with tele seismic inversions and back projection results. We demonstrated in that chapter that inclusion of the velocity time series produced by the seismogeodetic solution sharpens the picture of the source. We relied on Akaike's Bayesian information criterion for determining optimal smoothing, thus largely removing the human element from the decision-making for the inversion parameters. The seimogeodetic kinematic inversion could potentially be available within minutes of rupture initiation of a large event. We also showed that rupture models from the seimogeodetic data can provide insights into the source process.

In Chapter 5 we discussed the addition of offshore data int eh form of tide gauges, RTK GPS buoys and ocean bottom pressure sensors into the source inversion process. First we show with the example of the 2011 Tohoku-oki earthquake a source modeling approach that relies on joint inversion of land-based estimates of coseismic deformation from collocated GPS/accelerometer stations and ocean-based wave gauges. We have shown that the earthquake sources thus determined can be used as initial conditions to model tsunami propagation in the near-source coastline. We have compared the results of such models against post-event survey data and concluded that the inundation forecasts from such models could be used for tsunami early warning. We deduce that off-shore wave gauges in water shallower than previously thought (100-1000m) can be used for tsunami forecast. However we find that coastal tide gauges are of limited use for the event studied. We argue that in the future joint inversion of these near source geophysical data and deployment of further ocean-based instruments can help to deliver timely and accurate early warning in regions of tsunami hazard. We also show in Chapter 5 the effect of adding the off-shore data into he kinematic modeling process and produce a time dependent model of the source that relies on both the land- and ocean-based observables. The tsunami data increases the level of complexity int he shallow portion of the fault model, it increases the amplitude and duration of moment release in the shallow sub faults while leaving the deeper part of the model largely unchanged.

It is clear that employing more diverse geophysical data for modeling the earthquake source will aid in developing amore comprehensive image of the underlying processes. This stands to impact both our understanding of  large ruptures and our capability to respond to them rapidly. The ocean bottom stands to be the next frontier of seismological and geodetic exploration as technological improvements produce more and better instruments capable of measuring geophysical signals on the sea floor. 
